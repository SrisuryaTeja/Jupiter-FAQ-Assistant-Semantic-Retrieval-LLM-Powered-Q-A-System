{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7acb4b9",
   "metadata": {},
   "source": [
    "# Jupiter Community Forum Scraper\n",
    "\n",
    "This notebook contains an asynchronous web scraper using Playwright to:\n",
    "- Fetch topic URLs from categories and tags\n",
    "- Scrape individual topics for posts, user info, links, images, and tags\n",
    "- Save results to JSON files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fcda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "HELP_URL = 'https://community.jupiter.money/c/help/27'\n",
    "TAGS_URL = 'https://community.jupiter.money/tags'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9956c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Fetches all topic URLs by scrolling through the “Help” category page on Jupiter’s Discourse forum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af084b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_topic_urls_from_category(page):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        page: Playwright page instance\n",
    "    Returns:\n",
    "        set: Unique topic URLs from the help category\n",
    "    \"\"\"\n",
    "    topic_urls = set()\n",
    "    await page.goto(HELP_URL)\n",
    "\n",
    "    try:\n",
    "        await page.wait_for_selector(\"a.title\", timeout=10000)\n",
    "    except:\n",
    "        print(\"No topics found in help category\")\n",
    "        return topic_urls\n",
    "\n",
    "    previous_height = 0\n",
    "    while True:\n",
    "        # Scroll down\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await asyncio.sleep(3)\n",
    "\n",
    "        # Extract current topic URLs\n",
    "        topics = await page.query_selector_all(\"a.title\")\n",
    "        for t in topics:\n",
    "            href = await t.get_attribute(\"href\")\n",
    "            if href:\n",
    "                if href.startswith('http'):\n",
    "                    topic_urls.add(href)\n",
    "                else:\n",
    "                    topic_urls.add(\"https://community.jupiter.money\" + href)\n",
    "\n",
    "        # Check if new content was loaded\n",
    "        current_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        if current_height == previous_height:\n",
    "            break\n",
    "        previous_height = current_height\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771a4c6",
   "metadata": {},
   "source": [
    "### Fetches all tag page URLs from the Jupiter Community forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debede49",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_tags(page):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        page: Playwright page instance\n",
    "    Returns:\n",
    "        list: Tag page URLs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        await page.goto(TAGS_URL)\n",
    "        await page.wait_for_selector(\".discourse-tag.box\", timeout=10000)\n",
    "        tag_links = await page.query_selector_all(\".discourse-tag.box\")\n",
    "        tag_urls = []\n",
    "\n",
    "        for tag in tag_links:\n",
    "            href = await tag.get_attribute(\"href\")\n",
    "            if href:\n",
    "                if href.startswith('http'):\n",
    "                    tag_urls.append(href)\n",
    "                else:\n",
    "                    tag_urls.append(\"https://community.jupiter.money\" + href)\n",
    "\n",
    "        return tag_urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tags: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f25704",
   "metadata": {},
   "source": [
    "\n",
    "### This function automates scrolling through each tag-specific page on the Jupiter Community forum to extract all associated topic URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9bd2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_topic_urls_from_tags(page, tag_urls):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        page: Playwright page instance\n",
    "        tag_urls: List of tag page URLs\n",
    "    Returns:\n",
    "        set: Unique topic URLs from all tag pages\n",
    "    \"\"\"\n",
    "    topic_urls = set()\n",
    "    for tag_url in tag_urls:\n",
    "        print(f\"Scrolling through tag: {tag_url}\")\n",
    "        await page.goto(tag_url)\n",
    "        try:\n",
    "            await page.wait_for_selector(\"a.title\", timeout=10000)\n",
    "        except:\n",
    "            print(f\"No topics found in tag: {tag_url}\")\n",
    "            continue\n",
    "\n",
    "        previous_height = 0\n",
    "        while True:\n",
    "            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            await asyncio.sleep(2)\n",
    "\n",
    "            topics = await page.query_selector_all(\"a.title\")\n",
    "            for t in topics:\n",
    "                href = await t.get_attribute(\"href\")\n",
    "                if href:\n",
    "                    if href.startswith(\"http\"):\n",
    "                        topic_urls.add(href)\n",
    "                    else:\n",
    "                        topic_urls.add(\"https://community.jupiter.money\" + href)\n",
    "\n",
    "            current_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            if current_height == previous_height:\n",
    "                break\n",
    "            previous_height = current_height\n",
    "\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd5bf4f",
   "metadata": {},
   "source": [
    "### Extracts the name and title of the user who authored a forum post from a `.topic-body` element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea5a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_user_info(topic_body_element):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        topic_body_element: Playwright element handle for the .topic-body\n",
    "    Returns:\n",
    "        dict: {'name': str, 'title': str}\n",
    "    \"\"\"\n",
    "    user_obj = {'name': '', 'title': 'User'}\n",
    "    try:\n",
    "        name_el = await topic_body_element.query_selector('.first.full-name a')\n",
    "        if name_el:\n",
    "            user_obj['name'] = (await name_el.text_content()).strip()\n",
    "\n",
    "        title_el = await topic_body_element.query_selector('.user-title')\n",
    "        if title_el:\n",
    "            user_obj['title'] = (await title_el.text_content()).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting user info: {e}\")\n",
    "    return user_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb2187",
   "metadata": {},
   "source": [
    "The `scrape_topic` function is an **asynchronous web scraping utility** built with [Playwright](https://playwright.dev/). It scrapes structured data from a discussion topic page on a forum-like website (such as Discourse-based forums)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_topic(page, url):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        page: Playwright page instance\n",
    "        url: Topic URL\n",
    "    Returns:\n",
    "        dict: Scraped data with keys title, url, text, images, links, replies, posts, tags\n",
    "    \"\"\"\n",
    "    try:\n",
    "        await page.goto(url, timeout=30000)\n",
    "        await page.wait_for_selector('.topic-body', timeout=15000)\n",
    "\n",
    "        # Title extraction\n",
    "        title = ''\n",
    "        try:\n",
    "            title_el = await page.wait_for_selector('.fancy-title', timeout=5000)\n",
    "            title = await title_el.text_content()\n",
    "        except:\n",
    "            alt_el = await page.query_selector('h1')\n",
    "            if alt_el:\n",
    "                title = await alt_el.text_content()\n",
    "\n",
    "        # Posts extraction\n",
    "        topic_bodies = await page.query_selector_all('.topic-body')\n",
    "        posts = []\n",
    "        for idx, body in enumerate(topic_bodies):\n",
    "            user_info = await extract_user_info(body)\n",
    "            post_text = ''\n",
    "            cooked = await body.query_selector('.cooked')\n",
    "            if cooked:\n",
    "                post_text = (await cooked.text_content()).strip()\n",
    "            post_type = 'question' if idx == 0 else 'reply'\n",
    "            if user_info['name'] and post_text:\n",
    "                posts.append({\n",
    "                    'user': user_info,\n",
    "                    'text': post_text,\n",
    "                    'post_type': post_type,\n",
    "                    'post_index': idx\n",
    "                })\n",
    "\n",
    "        # Links and images\n",
    "        links = [await a.get_attribute('href') for a in await page.query_selector_all('.topic-body .cooked a') if await a.get_attribute('href')]\n",
    "        images = [await img.get_attribute('src') for img in await page.query_selector_all('.topic-body .cooked img') if await img.get_attribute('src')]\n",
    "\n",
    "        # Tags\n",
    "        tags = [await t.text_content() for t in await page.query_selector_all('.title-wrapper .discourse-tag.box')]\n",
    "\n",
    "        # Separate question and replies\n",
    "        question_text = next((p['text'] for p in posts if p['post_type']=='question'), '')\n",
    "        replies = [{'user': p['user']['name'], 'text': p['text']} for p in posts if p['post_type']=='reply']\n",
    "\n",
    "        return {'title': title.strip(), 'url': url, 'text': question_text, 'images': images,\n",
    "                'links': links, 'replies': replies, 'posts': posts, 'tags': tags}\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b2827",
   "metadata": {},
   "source": [
    "The `main` function is the asynchronous entry point for scraping Jupiter's community forum. It uses **Playwright** to automate browsing and extract FAQ-related data from discussion threads across both the \"Help\" category and tag-filtered views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(test_mode=False, max_pages=5):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])\n",
    "        page = await browser.new_page()\n",
    "        page.set_viewport_size({\"width\": 1920, \"height\": 1080})\n",
    "        await page.set_extra_http_headers({\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "\n",
    "        help_urls = await get_topic_urls_from_category(page)\n",
    "        tag_urls = await get_tags(page)\n",
    "        tag_topic_urls = await get_topic_urls_from_tags(page, tag_urls)\n",
    "        all_urls = list(help_urls.union(tag_topic_urls))\n",
    "        if test_mode:\n",
    "            all_urls = all_urls[:max_pages]\n",
    "\n",
    "        results, failed = [], []\n",
    "        for i, url in enumerate(all_urls):\n",
    "            print(f\"Scraping {i+1}/{len(all_urls)}: {url}\")\n",
    "            res = await scrape_topic(page, url)\n",
    "            if res: results.append(res)\n",
    "            else: failed.append(url)\n",
    "            if i % 10 == 0: await asyncio.sleep(2)\n",
    "\n",
    "        # Save outputs\n",
    "        out_file = 'faq_data_test.json' if test_mode else 'faq_data_raw.json'\n",
    "        fail_file = 'failed_urls_test.json' if test_mode else 'failed_urls.json'\n",
    "        with open(out_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        with open(fail_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(failed, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved {len(results)} topics to {out_file}\")\n",
    "\n",
    "# Entry point\n",
    "TEST_MODE = False  # set to False to scrape all pages\n",
    "\n",
    "# Run the scraper\n",
    "asyncio.run(main(test_mode=TEST_MODE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
