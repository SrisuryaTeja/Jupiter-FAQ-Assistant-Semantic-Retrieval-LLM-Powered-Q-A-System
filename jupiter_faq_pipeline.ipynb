{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58106531",
   "metadata": {},
   "source": [
    "# Jupiter Money FAQ Pipeline Notebook\n",
    "\n",
    "This notebook implements the end-to-end pipeline for building an interactive FAQ assistant using an open-source LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68861c17",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "Install required packages and import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc78b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install (run once):\n",
    "# !pip install playwright nest_asyncio sentence-transformers faiss-cpu langdetect googletrans==4.0.0-rc1 python-dotenv requests\n",
    "# !playwright install\n",
    "\n",
    "import os\n",
    "import re\n",
    "import html\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import asyncio\n",
    "import json\n",
    "from playwright.async_api import async_playwright\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "dotenv_path = find_dotenv()\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# ── Configure Your Hosted Open-Source LLM API ────────────────────────────────\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "MODEL_NAME = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "MULTILINGUAL_MODEL = \"paraphrase-multilingual-MiniLM-L12-v2\" \n",
    "\n",
    "translator = Translator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab34e1d",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning & Preparation\n",
    "Load raw JSON, clean text, and prepare question/metadata lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f4524ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_JSON       = r\"D:\\srisurya\\Ml_projects\\Faq chat bot\\faq_data_raw.json\"\n",
    "INDEX_FILE     = r\"D:\\srisurya\\Ml_projects\\Faq chat bot\\faq.index\"\n",
    "QUESTIONS_FILE = r\"D:\\srisurya\\Ml_projects\\Faq chat bot\\faq_questions.json\"\n",
    "METADATA_FILE  = r\"D:\\srisurya\\Ml_projects\\Faq chat bot\\faq_metadata.json\"\n",
    "EMBEDDINGS_FILE = r\"D:\\srisurya\\Ml_projects\\Faq chat bot\\faq_embeddings.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefba4ef",
   "metadata": {},
   "source": [
    "### 🧹 Data Cleaning and Preparation\n",
    "\n",
    "This section defines two key functions to clean and structure the raw FAQ dataset:\n",
    "\n",
    "---\n",
    "\n",
    "#### **`clean_text(text: str) -> str`**\n",
    "\n",
    "Cleans raw text data by:\n",
    "- Decoding HTML entities (e.g., `&amp;` → `&`)\n",
    "- Replacing multiple spaces/newlines with a single space\n",
    "- Removing mentions (e.g., `@username`)\n",
    "- Removing emojis and custom shortcodes (e.g., `:smile:`)\n",
    "- Removing URLs\n",
    "- Stripping leading/trailing non-word characters\n",
    "\n",
    "> Returns a simplified, cleaned version of the input string.\n",
    "\n",
    "---\n",
    "\n",
    "#### **`load_and_prepare_data()`**\n",
    "\n",
    "Loads the raw scraped JSON (`RAW_JSON`) and prepares it for embedding and retrieval:\n",
    "- Cleans the question text and skips duplicates\n",
    "- Cleans each post (question + replies)\n",
    "- Collects structured metadata for each topic, including:\n",
    "  - Cleaned title\n",
    "  - URL\n",
    "  - Tags\n",
    "  - Cleaned posts list (`text`, `user`, etc.)\n",
    "\n",
    "> Prints the number of unique, cleaned questions loaded.  \n",
    "> Returns two lists:\n",
    "> - `questions`: List of cleaned question strings  \n",
    "> - `metadatas`: Corresponding metadata dictionaries\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "521e339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'@[\\w\\-]+', '', text)\n",
    "    text = re.sub(r':[\\w\\-]+:', '', text)\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'^\\W+|\\W+$', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    with open(RAW_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    questions, metadatas = [], []\n",
    "    seen = set()\n",
    "    for topic in data:\n",
    "        q = clean_text(topic.get(\"text\", \"\"))\n",
    "        if not q or q in seen:\n",
    "            continue\n",
    "        seen.add(q)\n",
    "\n",
    "        # clean posts\n",
    "        cleaned_posts = []\n",
    "        for p in topic.get(\"posts\", []):\n",
    "            cp = p.copy()\n",
    "            cp[\"text\"] = clean_text(cp.get(\"text\", \"\"))\n",
    "            cleaned_posts.append(cp)\n",
    "\n",
    "        questions.append(q)\n",
    "        metadatas.append({\n",
    "            \"tags\":  topic.get(\"tags\", []),\n",
    "            \"title\": clean_text(topic.get(\"title\", \"\")),\n",
    "            \"url\":   topic.get(\"url\", \"\"),\n",
    "            \"posts\": cleaned_posts,\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(questions)} unique questions.\")\n",
    "    return questions, metadatas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aafa3f3",
   "metadata": {},
   "source": [
    "### 🌐 Language Normalization\n",
    "\n",
    "#### `ensure_english(text: str) -> str`\n",
    "\n",
    "This function ensures that input text is in **English**, translating it if necessary.\n",
    "\n",
    "---\n",
    "\n",
    "**Functionality:**\n",
    "- Detects the language of the input string using `langdetect`.\n",
    "- If the detected language is not English (`'en'`), it uses **Google Translate** (`googletrans`) to convert it to English.\n",
    "- If detection fails or the text is already in English, it returns the original text unchanged.\n",
    "\n",
    "---\n",
    "\n",
    "**Purpose:**\n",
    "To standardize multilingual user content by translating all input text into English before further processing (e.g., indexing, embedding, or retrieval).\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "- The function gracefully handles errors—if detection or translation fails, it returns the input text as-is.\n",
    "- Requires internet access for translation via Google Translate.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dbbb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_english(text: str) -> str:\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        if lang != 'en':\n",
    "            return translator.translate(text, dest='en').text\n",
    "    except:\n",
    "        pass\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5951c7",
   "metadata": {},
   "source": [
    "### 🧠 Building or Loading FAISS Index & Embeddings\n",
    "\n",
    "#### `build_or_load_index_and_embeddings(questions, metadatas)`\n",
    "\n",
    "This function prepares the semantic search backend by either:\n",
    "- **Loading** a precomputed FAISS index and embeddings from disk (if available), or\n",
    "- **Building** a new index from scratch using the provided questions.\n",
    "\n",
    "---\n",
    "\n",
    "####  Function Responsibilities:\n",
    "- Initializes a `SentenceTransformer` model (`MULTILINGUAL_MODEL`).\n",
    "- Checks if existing files (`INDEX_FILE`, `EMBEDDINGS_FILE`, `QUESTIONS_FILE`, `METADATA_FILE`) exist:\n",
    "  - If yes, loads and returns saved index, embeddings, questions, and metadata.\n",
    "  - If no, computes new embeddings using the model:\n",
    "    - Normalizes them for cosine similarity.\n",
    "    - Builds a **FAISS index** using inner product (IP) search.\n",
    "    - Saves index, embeddings, questions, and metadata to disk for reuse.\n",
    "\n",
    "---\n",
    "\n",
    "####  Returns:\n",
    "- `idx`: FAISS index (`IndexFlatIP`)\n",
    "- `embs`: Numpy array of question embeddings\n",
    "- `questions`: List of question texts\n",
    "- `metadatas`: Associated metadata for each question\n",
    "- `model_multi`: The multilingual SentenceTransformer model\n",
    "\n",
    "---\n",
    "\n",
    "####  Caching:\n",
    "This function allows your notebook to resume quickly between runs by persisting data to disk—saving significant time for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### Tip:\n",
    "You can change the model used by modifying the `MULTILINGUAL_MODEL` constant, e.g., to `\"all-MiniLM-L6-v2\"` for English-only datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e521f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_or_load_index_and_embeddings(questions, metadatas):\n",
    "    model_multi = SentenceTransformer(MULTILINGUAL_MODEL)\n",
    "    if os.path.exists(INDEX_FILE) and os.path.exists(EMBEDDINGS_FILE):\n",
    "        idx = faiss.read_index(INDEX_FILE)\n",
    "        embs = np.load(EMBEDDINGS_FILE)\n",
    "        qs = json.load(open(QUESTIONS_FILE)); ms = json.load(open(METADATA_FILE))\n",
    "        print(\"Loaded existing index and embeddings.\")\n",
    "        return idx, embs, qs, ms, model_multi\n",
    "    embs = model_multi.encode(questions, convert_to_numpy=True, batch_size=32)\n",
    "    faiss.normalize_L2(embs)\n",
    "    idx = faiss.IndexFlatIP(embs.shape[1]); idx.add(embs)\n",
    "    faiss.write_index(idx, INDEX_FILE); np.save(EMBEDDINGS_FILE, embs)\n",
    "    json.dump(questions, open(QUESTIONS_FILE, \"w\"), indent=2)\n",
    "    json.dump(metadatas, open(METADATA_FILE, \"w\"), indent=2)\n",
    "    print(f\"Built index with {idx.ntotal} vectors.\")\n",
    "    return idx, embs, questions, metadatas, model_multi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3573c48",
   "metadata": {},
   "source": [
    "###  Semantic Similarity Suggestion\n",
    "\n",
    "#### `suggest_related(query, questions, idx, model, embeddings, top_k=5)`\n",
    "\n",
    "This function retrieves past questions that are semantically similar to the user’s current query, using FAISS for efficient nearest neighbor search.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Function Workflow:\n",
    "\n",
    "1. **Track the query**  \n",
    "   Appends the incoming query to a `query_history` list for later reference or analytics.\n",
    "\n",
    "2. **Encode the query**  \n",
    "   Transforms the query into a vector using a pretrained `SentenceTransformer` model (`model`), and normalizes it for cosine similarity search.\n",
    "\n",
    "3. **Search the FAISS index**  \n",
    "   Runs an inner-product (IP) search on the full FAISS index (`idx`) of all past embeddings to find the top-`k` most similar questions.\n",
    "\n",
    "4. **Return suggestions**  \n",
    "   Maps the result indices back to original question strings.\n",
    "\n",
    "---\n",
    "\n",
    "####  Parameters:\n",
    "- `query`: New input string for which related past questions are to be found.\n",
    "- `questions`: List of all indexed questions.\n",
    "- `idx`: FAISS index (built with `IndexFlatIP` and normalized embeddings).\n",
    "- `model`: SentenceTransformer used to encode the query.\n",
    "- `embeddings`: Numpy array of all embedded questions.\n",
    "- `top_k`: Number of suggestions to return (default = 5).\n",
    "\n",
    "---\n",
    "\n",
    "####  Returns:\n",
    "- A list of `top_k` most semantically similar past questions from the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b67e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_history = []\n",
    "\n",
    "def suggest_related(\n",
    "    query: str,\n",
    "    questions: list,\n",
    "    idx: faiss.IndexFlatIP,\n",
    "    model: SentenceTransformer,\n",
    "    embeddings: np.ndarray,\n",
    "    top_k: int = 5\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Suggest past questions semantically similar to `query`.\n",
    "    Uses the full FAISS index `idx` over `embeddings`.\n",
    "    \"\"\"\n",
    "    query_history.append(query)\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = idx.search(q_emb, top_k)\n",
    "    scores, ids = D[0], I[0]\n",
    "    suggestions = [questions[i] for i in ids]\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc7235",
   "metadata": {},
   "source": [
    "### 🏷️ Tag-Based Filtering with LLM-Aided Classification\n",
    "\n",
    "This section implements intelligent retrieval based on tag prediction using an open-source LLM and tag-filtered FAISS indexing.\n",
    "\n",
    "---\n",
    "\n",
    "#### `classify_tags_with_open_llm(query: str, possible_tags: list) -> list`\n",
    "\n",
    "Uses an **Open LLM API (Together.xyz)** to classify a user query into 2–3 relevant tags.\n",
    "\n",
    "**How it works:**\n",
    "- Sends a prompt to the LLM with a list of possible tags and a user query.\n",
    "- The model returns a comma-separated list of predicted tags.\n",
    "- If the query is off-topic or ambiguous, it returns `\"none\"`.\n",
    "\n",
    "**Returns:**\n",
    "- A list of tags (max 3) or an empty list (`[]`) if none apply.\n",
    "\n",
    "> 📌 This enables dynamic filtering of FAQs based on content themes.\n",
    "\n",
    "---\n",
    "\n",
    "#### `retrieve_with_prefilter(...) -> list`\n",
    "\n",
    "Performs **semantic search with tag filtering**. Only retrieves similar questions whose metadata contains the predicted tags.\n",
    "\n",
    "**Inputs:**\n",
    "- `query`: The user’s search string\n",
    "- `predicted_tags`: Tags predicted by the LLM\n",
    "- `questions`: List of all questions\n",
    "- `metadatas`: Metadata per question (tags, title, posts, etc.)\n",
    "- `model`: SentenceTransformer used for encoding\n",
    "- `embeddings`: FAISS-compatible numpy embeddings\n",
    "- `top_k`: Number of results to retrieve (default = 10)\n",
    "\n",
    "---\n",
    "\n",
    "**Workflow:**\n",
    "\n",
    "1. **Filter candidate questions** using the predicted tags.\n",
    "2. **Build a temporary FAISS index** using only the filtered subset.\n",
    "3. **Encode the query** and perform semantic search on the sub-index.\n",
    "4. **Return results** with question text, score, tags, title, URL, and full post content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a4d81a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tags_with_open_llm(query: str, possible_tags: list) -> list:\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant that assigns relevant tags to user questions about Jupiter.money. If the question is unrelated or ambiguous, respond with \"none\".\n",
    "\n",
    "Possible tags: {', '.join(possible_tags)}\n",
    "\n",
    "Classify this question into the most relevant 2–3 tags. If it is unrelated, say \"none\".\n",
    "\n",
    "Question:\n",
    "\"{query}\"\n",
    "\n",
    "Respond with a comma-separated list of tags or \"none\".\n",
    "\"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://api.together.xyz/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 100,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    content = content.strip().lower()\n",
    "    if \"none\" in content:\n",
    "        return []\n",
    "    return [tag.strip() for tag in content.split(\",\") if tag.strip()]\n",
    "\n",
    "def retrieve_with_prefilter(\n",
    "    query: str,\n",
    "    predicted_tags: list,\n",
    "    questions: list,\n",
    "    metadatas: list,\n",
    "    model: SentenceTransformer,\n",
    "    embeddings: np.ndarray,\n",
    "    top_k: int = 10\n",
    "):\n",
    "    \n",
    "    if predicted_tags:\n",
    "        allowed_ids = [\n",
    "            i for i, meta in enumerate(metadatas)\n",
    "            if any(tag in meta[\"tags\"] for tag in predicted_tags)\n",
    "        ]\n",
    "    else:\n",
    "        allowed_ids = [i for i, meta in enumerate(metadatas) if not meta[\"tags\"]]\n",
    "\n",
    "    if not allowed_ids:\n",
    "        return []\n",
    "\n",
    "  \n",
    "    sub_embs = embeddings[allowed_ids]\n",
    "    faiss.normalize_L2(sub_embs)\n",
    "    dim = sub_embs.shape[1]\n",
    "    sub_index = faiss.IndexFlatIP(dim)\n",
    "    sub_index.add(sub_embs)\n",
    "\n",
    "\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "\n",
    "    D, I = sub_index.search(q_emb, top_k)\n",
    "    scores, idxs = D[0], I[0]\n",
    "\n",
    "\n",
    "    results = []\n",
    "    for score, sub_i in zip(scores, idxs):\n",
    "        orig_i = allowed_ids[sub_i]\n",
    "        md = metadatas[orig_i]\n",
    "        results.append({\n",
    "            \"question\": questions[orig_i],\n",
    "            \"score\":    float(score),\n",
    "            \"tags\":     md[\"tags\"],\n",
    "            \"title\":    md[\"title\"],\n",
    "            \"url\":      md[\"url\"],\n",
    "            \"posts\":    md[\"posts\"],\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038227f4",
   "metadata": {},
   "source": [
    "### 🤖 Answer Generation with Open LLM\n",
    "\n",
    "#### `generate_answer_with_open_llm(query: str, context_posts: list) -> str`\n",
    "\n",
    "This function uses an **open-source LLM (via Together.xyz)** to generate a helpful, natural-language answer based on selected forum discussion posts.\n",
    "\n",
    "---\n",
    "\n",
    "#### 💡 Function Purpose:\n",
    "To synthesize a concise and friendly response to the user's query using real community discussions as context.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 How it works:\n",
    "\n",
    "1. **Context Building:**\n",
    "   - Extracts up to the top 3 relevant posts from `context_posts`.\n",
    "   - Formats them as `\"username: post text\"` to simulate a forum thread.\n",
    "\n",
    "2. **Prompt Construction:**\n",
    "   - Uses a **system prompt** that guides the model to behave like a polite and clear Jupiter assistant.\n",
    "   - Embeds the user’s question and the forum context into a **user prompt**.\n",
    "\n",
    "3. **LLM Call:**\n",
    "   - Sends the prompt to Together.xyz's chat API using the specified `MODEL_NAME`.\n",
    "   - Uses a moderate temperature (`0.5`) to keep responses helpful and creative, but grounded.\n",
    "\n",
    "4. **Fallback:**\n",
    "   - If no context is available, returns a default message encouraging the user to rephrase or visit the community.\n",
    "\n",
    "---\n",
    "\n",
    "#### Parameters:\n",
    "- `query`: The user’s input question.\n",
    "- `context_posts`: A list of relevant forum posts with structure `{ \"user\": { \"name\": ... }, \"text\": ... }`.\n",
    "\n",
    "#### Returns:\n",
    "- A single string: the model-generated answer to the user’s question, grounded in the forum content.\n",
    "\n",
    "---\n",
    "\n",
    "> ⚠️ Requires `TOGETHER_API_KEY` and internet access. Make sure the Together API is accessible and the model you're using is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c6e5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_open_llm(query: str, context_posts: list) -> str:\n",
    "    if not context_posts:\n",
    "        return (\"I'm not sure about that yet. You can try rephrasing your question \"\n",
    "                \"or visit the Jupiter Community for more help.\")\n",
    "\n",
    "    # Build context: top 3 posts\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"{post['user']['name']}: {post['text']}\"\n",
    "        for post in context_posts[:3]\n",
    "    )\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant for Jupiter.money.\\n\"\n",
    "        \"Use the provided forum discussion to answer the user's question clearly and politely.\\n\"\n",
    "        \"Rephrase responses in friendly, natural language.\\n\"\n",
    "        \"If you're not sure or the context is unrelated, say so gracefully.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"User question: {query}\\n\\n\"\n",
    "        f\"Forum context:\\n{context}\\n\\n\"\n",
    "        \"Answer the user based on the forum context:\"\n",
    "    )\n",
    "\n",
    "    resp = requests.post(\n",
    "        \"https://api.together.xyz/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\",   \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 512,\n",
    "        },\n",
    "    )\n",
    "    return resp.json()[\"choices\"][0][\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c02e4",
   "metadata": {},
   "source": [
    "### 🧪 Interactive QA Loop with Retrieval + Generation\n",
    "\n",
    "This section ties everything together into an interactive, end-to-end retrieval-augmented question-answering system for Jupiter.money forum data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧱 Step-by-Step Breakdown:\n",
    "\n",
    "1. **Data & Index Setup**\n",
    "   - `load_and_prepare_data()` loads and cleans the questions and metadata.\n",
    "   - `build_or_load_index_and_embeddings()` loads or creates a FAISS index and sentence embeddings.\n",
    "   - `all_tags` is created by extracting all unique tags from the metadata.\n",
    "\n",
    "2. **User Loop**\n",
    "   Continuously prompts the user to ask a question:\n",
    "\n",
    "   **a. Tag Prediction:**\n",
    "   - Uses `classify_tags_with_open_llm()` to assign the query to 2–3 relevant forum tags.\n",
    "   - Helps narrow down the search space to only relevant discussions.\n",
    "\n",
    "   **b. Contextual Retrieval:**\n",
    "   - `retrieve_with_prefilter()` fetches semantically similar posts from the predicted tag space.\n",
    "\n",
    "   **c. Answer Generation:**\n",
    "   - Two answers are generated using `generate_answer_with_open_llm()`:\n",
    "     1. **Retrieval-based answer**: Uses real community discussions as context.\n",
    "     2. **LLM-only answer**: Generates without any grounding/context.\n",
    "   - Latency for both responses is measured and printed.\n",
    "\n",
    "   **d. Related Questions:**\n",
    "   - `suggest_related()` retrieves other semantically similar past questions from the full corpus.\n",
    "   - These are printed as additional suggestions.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🖥️ Output:\n",
    "- ✅ Predicted tags\n",
    "- ✅ Two answer versions (with and without retrieval grounding)\n",
    "- ✅ Related questions list\n",
    "- ✅ Latency for both approaches (retrieval-augmented vs LLM-only)\n",
    "\n",
    "---\n",
    "\n",
    "> 💬 This setup helps evaluate the benefits of retrieval-augmented generation (RAG) and guides users to similar existing content.\n",
    "\n",
    "> 🔁 Type `\"exit\"` to end the loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1226 unique questions.\n",
      "Loaded existing index and embeddings.\n",
      "\n",
      " You asked:\n",
      "  Hi, as we know Jupiter has two accounts in the app with Federal Bank.  I don’t have any ongoing pot but somehow I transferred money to my second account using account details.  Now how can I transfer that money to my main savings account?\n",
      "\n",
      "Predicted tags: ['pots', 'savings-account', 'transfer']\n",
      "\n",
      "⏱ Retrieval-based (2.13s):\n",
      "Hello! It's great that you've reached out for help regarding transferring money between your accounts on Jupiter.money.\n",
      "\n",
      "Based on the discussion in the forum, you can follow these steps:\n",
      "\n",
      "1. Go to the \"POTS\" section in the Jupiter app.\n",
      "2. Look for the \"Withdraw money/delete Pots\" option.\n",
      "\n",
      "Alternatively, user Siddharth B R suggested using UPI (Unified Payments Interface) to transfer the funds. You can give that a try as well if the first method doesn't work for you.\n",
      "\n",
      "Please remember to double-check the account details before confirming the transfer to ensure the money goes to your desired main savings account. If you need further assistance or clarification, feel free to ask!\n",
      "\n",
      "Confidence: 85%\n",
      "\n",
      "⏱ LLM-only    (0.00s):\n",
      "I'm not sure about that yet. You can try rephrasing your question or visit the Jupiter Community for more help.\n",
      "\n",
      "💡 You might also be interested in these related questions:\n",
      "  • Hi, as we know Jupiter has two accounts in the app with Federal Bank. I don’t have any ongoing pot but somehow I transferred money to my second account using account details. Now how can I transfer that money to my main savings account\n",
      "  • Need help!!! Im currently using jupiter ac with my other number and made one with secondary number which i dont want to use it. Now im having a problem with applying loan Because i have 2 accounts. I want to delete the secondary AC\n",
      "  • I have done a payment yesterday the money has been debited from my jupiter account and the amount is still in processing and not yet credited to the beneficiary… i attach the image1170×2532 195 KB\n",
      "  • I my college want to apply for scholarship so they for some some documents and passbook but I have only Jupiter money account I have don’t have any passbook\n",
      "  • I’m Imran.I tried to open a savings account in jupiter.I am so disappointed as i couldn’t create account. I raised a ticket three months ago but still I didn’t get any solution from the team. This is my request id #1980571  I hope you can help me to create new account\n",
      "\n",
      " You asked:\n",
      "  नमस्ते, जैसा कि हम जानते हैं कि जुपिटर के ऐप में फेडरल बैंक के साथ दो खाते हैं।  मेरे पास कोई चालू खाता नहीं है, लेकिन किसी तरह मैंने खाता विवरण का उपयोग करके अपने दूसरे खाते में पैसे ट्रांसफर कर दिए।  अब मैं उस पैसे को अपने मुख्य बचत खाते में कैसे ट्रांसफर कर सकता हूँ?\n",
      "\n",
      "Predicted tags: ['federal', 'savings-account', 'transfer']\n",
      "\n",
      "⏱ Retrieval-based (2.85s):\n",
      "Hello! It's nice to see you're seeking help for transferring money within your Jupiter accounts. Based on the forum discussion, you can follow these steps:\n",
      "\n",
      "1. Go to the \"POTS\" section in the Jupiter app.\n",
      "2. Look for the \"Withdraw money/delete Pots\" option.\n",
      "\n",
      "Additionally, user Siddharth B R suggested using UPI for the transfer. However, since you've already transferred the money to your second account, it would be more appropriate to use the \"Withdraw money/delete Pots\" option as mentioned by ABDUL RAZACK K P.\n",
      "\n",
      "If you find any difficulties or have further questions, please let me know, and I'll be happy to help!\n",
      "\n",
      "Warm regards,\n",
      "Your Helpful Assistant for Jupiter.money\n",
      "\n",
      "⏱ LLM-only    (0.00s):\n",
      "I'm not sure about that yet. You can try rephrasing your question or visit the Jupiter Community for more help.\n",
      "\n",
      "💡 You might also be interested in these related questions:\n",
      "  • Hi, as we know Jupiter has two accounts in the app with Federal Bank. I don’t have any ongoing pot but somehow I transferred money to my second account using account details. Now how can I transfer that money to my main savings account\n",
      "  • Well I just opened the account and it’s asking me for my initial funding of 250 rs. I’m ok with it but I get to know about the debit card charges both virtual and physical so either way I have to pay for debit card. Now how can I close my account without getting stuck in these problems. I didn’t paid 250 rs initial funding still… Help please\n",
      "  • Need help!!! Im currently using jupiter ac with my other number and made one with secondary number which i dont want to use it. Now im having a problem with applying loan Because i have 2 accounts. I want to delete the secondary AC\n",
      "  • I know there is button to share account details. Here are some enhancements: 1 => Include SWIFT code in the text (this will help receiving money internationally) 2 => Instead of ‘Federal’, write ‘Federal Bank’ (this will help foreigners to understand bank name more quickly) Check attached image Screenshot_20221023-183249_WhatsApp1080×916 119 KB This is a minute change in codebase, only some strings will need to be changed\n",
      "  • Curious to know 2 things. What is the tool that you are using to manage  Is it something you guys built or a 3rd party tool? Which is the main bank you have tie-up with to issue bank account\n",
      "\n",
      " You asked:\n",
      "  \n",
      "\n",
      "Predicted tags: none\n",
      "\n",
      "⏱ Retrieval-based (2.42s):\n",
      "Based on the forum discussion, it seems like Chad Pitt has received an email from a .money domain and is unsure whether it's legitimate or not. In response to Chad's question, I would suggest that it's generally a good idea to consult with a legal or financial advisor before responding to any financial or legal matters. This can help ensure that you're making informed decisions and protecting yourself from any potential risks. However, I would also recommend taking a closer look at the email and examining any details or red flags that might indicate its legitimacy. Ultimately, the best course of action will depend on the specifics of the email and Chad's individual situation.\n",
      "\n",
      "⏱ LLM-only    (0.00s):\n",
      "I'm not sure about that yet. You can try rephrasing your question or visit the Jupiter Community for more help.\n",
      "\n",
      "💡 You might also be interested in these related questions:\n",
      "  • IMG_20220210_1243391080×2246 364 KB\n",
      "  • IMG_20230110_153254720×1459 164 KB\n",
      "  • Screenshot_2022-11-11-17-32-02-13_1c337646f29875672b5a61192b9010f91080×1897 253 KB We just couldn’t hold back And yes, it’s a collab with Marvel Check out our new logo too\n",
      "  • IMG_09361920×1730 222 KB\n",
      "  • Screenshot_2023-07-19-19-17-15-03_cf3cf72bd8e53b0db7ddb0a6f2208af9720×542 58 KB\n"
     ]
    }
   ],
   "source": [
    "questions, metadatas = load_and_prepare_data()\n",
    "idx, embs, questions, metadatas, model = build_or_load_index_and_embeddings(questions, metadatas)\n",
    "all_tags = sorted({t for m in metadatas for t in m[\"tags\"]})\n",
    "while True:\n",
    "        q = input(\"Ask a question (exit to quit): \").strip()\n",
    "        if q.lower() == \"exit\": break\n",
    "        print(f\"\\n You asked:\\n  {q}\\n\")\n",
    "        tags = classify_tags_with_open_llm(q, all_tags)\n",
    "        print(f\"Predicted tags: {tags or 'none'}\")\n",
    "        hits = retrieve_with_prefilter(q, tags, questions, metadatas, model, embs, top_k=8)\n",
    "        # 1) Retrieval-based\n",
    "        start = time.time()\n",
    "        ret_answer = generate_answer_with_open_llm(q, hits[0][\"posts\"] if hits else [])\n",
    "        ret_latency = time.time() - start\n",
    "        \n",
    "        # 2) LLM-only (no retrieval context)\n",
    "        start = time.time()\n",
    "        llm_only_answer = generate_answer_with_open_llm(q, [])\n",
    "        llm_latency = time.time() - start\n",
    "        \n",
    "        # 3) Display comparison\n",
    "        print(f\"\\nRetrieval-based ({ret_latency:.2f}s):\\n{ret_answer}\\n\")\n",
    "        print(f\" LLM-only    ({llm_latency:.2f}s):\\n{llm_only_answer}\\n\")\n",
    "        suggestions = suggest_related(q, questions, idx, model, embs, top_k=5)\n",
    "        print(\"You might also be interested in these related questions:\")\n",
    "        for s in suggestions:\n",
    "            print(\"  •\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c0da51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
